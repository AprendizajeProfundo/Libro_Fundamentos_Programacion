{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "median-southeast",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:#F72585\">Diferenciación automática con torch.autograd</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-viewer",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-business",
   "metadata": {},
   "source": [
    "Al entrenar redes neuronales, el algoritmo más utilizado es la propagación hacia atrás (**back propagation**). En este algoritmo, los parámetros (pesos del modelo) se ajustan de acuerdo con el gradiente de la función de pérdida con respecto al parámetro dado.\n",
    "\n",
    "Para calcular esos gradientes, PyTorch tiene un motor de diferenciación incorporado llamado `torch.autograd`. Admite el cálculo automático del gradiente para cualquier gráfo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-neighborhood",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Tensores, funciones y grafo computacional</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-queens",
   "metadata": {},
   "source": [
    "Considere la red neuronal de una capa más simple, con entrada $x$, parámetros $w$ y $b$, y alguna función de pérdida. Se puede definir en PyTorch de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impressive-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # entrada\n",
    "y = torch.Tensor([0, 1, 0]) # etiqueta (valor verdadero)\n",
    "w = torch.randn(5,3, requires_grad=True) # matriz de pesos\n",
    "b = torch.randn(3, requires_grad=True)# bias\n",
    "z = torch.matmul(x,w)+b # calculo de la capa\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee03538e-e212-4df8-9bc5-3728946bf8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=  tensor([1., 1., 1., 1., 1.])\n",
      "y=  tensor([0., 1., 0.])\n",
      "w=  tensor([[-0.2402, -0.1829, -1.4698],\n",
      "        [-1.5692, -0.8134,  1.4860],\n",
      "        [-0.1677, -0.7561,  1.3061],\n",
      "        [-0.7970, -1.0626,  0.7416],\n",
      "        [ 0.6841, -1.0704, -0.2315]], requires_grad=True)\n",
      "b=  tensor([0.9137, 1.3851, 0.3073], requires_grad=True)\n",
      "z=  tensor([-1.1763, -2.5003,  2.1397], grad_fn=<AddBackward0>)\n",
      "loss=  tensor(1.6997, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('x= ', x)\n",
    "print('y= ',y)\n",
    "print('w= ', w)\n",
    "print('b= ', b)\n",
    "print('z= ', z)\n",
    "print('loss= ', loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-rugby",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Entropía Cruzada</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-tuesday",
   "metadata": {},
   "source": [
    "Recuerde que si $p=(p_1, p_2, p_3)$ y $q=(q_1, q_2, q_3)$ son dos distribuciones de probabilidad, de las cuales una se supone la verdadera, digamos $q$ y la otra una aproximación, en este caso $p$., entonces la entropía cruzada entre $p$ y $q$ se define mediante\n",
    "\n",
    "$$\n",
    "\\mathfrak{L}(p,q) = - (q_1 \\log p_1 + q_2 \\log p_2 + q_3 \\log p_3)\n",
    "$$\n",
    "\n",
    "La entropía cruzada mide que tanto se parece la disribución $p$ a la distribución $q$. En el ejemplo se tiene que $p = \\text{softmax(z)}$ y además $q=y$. Observe que por ejemplo $z_1 = x^T w_1$, en donde $w_1$ es la columna 1 de $w$. Note que además $p$ es función de los parámetros $w$ y $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-above",
   "metadata": {},
   "source": [
    "Este código define el siguiente grafo computacional:\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Libro_Fundamentos_Programacion/main/Pytorch/Imagenes/comp-graph.png\" width=\"800\" height=\"600\"/>\n",
    "\n",
    "</figure>\n",
    "\n",
    "Grafo computacional del cálculo descrito arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-forum",
   "metadata": {},
   "source": [
    "La propiedad `requires_grad` indica que son variables con respecto a las cuales se desea calcular el gradiente de la función *loss*.\n",
    "\n",
    "No es necesario declararlas como tal desde el comienzo. Puede hacerlo luego mediante el método\n",
    "\n",
    "+ x.requires_grad(True).\n",
    "\n",
    "Una función aplicada a tensores de un objeto de la clase `Function`, la cual viene equipada con lo necesario para la diferenciación automática. Una referencia a la función gradiente (back propagation) se almacena en `grad_fn`. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brutal-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f9378df53d0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x7f9378df53a0>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-culture",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Cálculo de gradientes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-demonstration",
   "metadata": {},
   "source": [
    "Vamos a calcular $\\frac{\\partial loss} {\\partial w}$ y $\\frac{\\partial loss} {\\partial b}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "practical-recruitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad=  tensor([[ 0.0786, -0.3081,  0.2982],\n",
      "        [ 0.0786, -0.3081,  0.2982],\n",
      "        [ 0.0786, -0.3081,  0.2982],\n",
      "        [ 0.0786, -0.3081,  0.2982],\n",
      "        [ 0.0786, -0.3081,  0.2982]])\n",
      "b.grad=  tensor([ 0.0786, -0.3081,  0.2982])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print('w.grad= ', w.grad)\n",
    "print('b.grad= ', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000a91f",
   "metadata": {},
   "source": [
    "Admonition\n",
    "```{admonition} Nota\n",
    ":class: \n",
    "- Solo podemos obtener las propiedades *grad* para los nodos hoja del grafo computacional, que tienen la propiedad *require_grad* establecida en *True*. Para todos los demás nodos de nuestro gráfico, los degradados no estarán disponibles.\n",
    "- Solo podemos realizar cálculos de gradiente usando hacia atrás una vez en un gráfico dado, por razones de rendimiento. Si necesitamos hacer varias llamadas hacia atrás en el mismo gráfico, debemos pasar *retain_graph = True* a la llamada hacia atrás.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-triple",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Deshabilitar el seguimiento de gradientes</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-direction",
   "metadata": {},
   "source": [
    " De forma predeterminada,  para todos los tensores con *require_grad = True* se está rastreando su historial computacional y admiten el cálculo del gradiente. Sin embargo, hay algunos casos en los que no necesitamos hacer eso, por ejemplo, cuando hemos entrenado el modelo y solo queremos aplicarlo a algunos datos de entrada, es decir, solo queremos hacer cálculos reenviados a través de la red. Podemos detener el seguimiento de los cálculos rodeando nuestro código de cálculo con el bloque  `torch.no_grad ()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-still",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-bahamas",
   "metadata": {},
   "source": [
    "Alternativamente se puede usar el método `detach`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-slovenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-basics",
   "metadata": {},
   "source": [
    "Existen motivos por los que quizás desee deshabilitar el seguimiento de gradientes:\n",
    "\n",
    "- Para marcar algunos parámetros en su red neuronal como parámetros congelados. Este es un escenario muy común para ajustar una red previamente entrenada (finetunning)\n",
    "- Para acelerar los cálculos cuando solo está haciendo un paso hacia adelante, porque los cálculos en tensores que no siguen los gradientes serían más eficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-neighbor",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Más sobre grafos computacionales</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-label",
   "metadata": {},
   "source": [
    "Conceptualmente, *autograd* mantiene un registro de datos (tensores) y todas las operaciones ejecutadas (junto con los nuevos tensores resultantes) en un gráfico acíclico dirigido (DAG) que consta de objetos de tipo *Function*. En este DAG, las hojas son los tensores de entrada, las raíces son los tensores de salida. Al trazar este gráfico desde las raíces hasta las hojas, puede calcular automáticamente los gradientes usando la regla de la cadena.\n",
    "\n",
    "En un paso hacia adelante (foreward), *autograd* hace dos cosas simultáneamente:\n",
    "\n",
    "- ejecuta la operación solicitada para calcular un tensor resultante\n",
    "- mantiene la función de gradiente de la operación en el DAG.\n",
    "\n",
    "El paso hacia atrás(backward) comienza cuando se llama a `.backward()` en la raíz del DAG. `autograd` entonces:\n",
    "\n",
    "- calcula los gradientes de cada `.grad_fn`,\n",
    "- los acumula en el atributo `.grad` del tensor respectivo\n",
    "- utilizando la regla de la cadena, se propaga hasta los tensores de las hojas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3655a586",
   "metadata": {},
   "source": [
    "```{admonition} Nota\n",
    ":class: \n",
    "Los DAG son dinámicos en PyTorch. Una cosa importante a tener en cuenta es que el gráfico se recrea desde cero; después de cada llamada .backward (), autograd comienza a completar un nuevo grafo. Esto es exactamente lo que le permite utilizar declaraciones de flujo de control en su modelo; puede cambiar la forma, el tamaño y las operaciones en cada iteración si es necesario.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-series",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Referencias</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-burden",
   "metadata": {},
   "source": [
    "1. Basado en los [tutoriales de Pytorch](https://pytorch.org/tutorials/)\n",
    "1. [Deep learning for coders with FastAI and Pytorch](http://library.lol/main/F13E85845AE48D9FD7488FE7630A9FD3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
