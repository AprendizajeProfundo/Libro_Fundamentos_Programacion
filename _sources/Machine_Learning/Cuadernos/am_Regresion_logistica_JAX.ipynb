{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#F72585\">Modelo Lineal de Clasificación  con JAX</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Introducción</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con su versión actualizada de [Autograd](https://github.com/hips/autograd), [JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html) puede diferenciar automáticamente el código nativo de Python y NumPy. Puede derivarse a través de un gran subconjunto de características de Python, incluidos bucles, condicionales, recursión y closures, e incluso puede tomar derivadas de derivadas de derivadas. Admite la diferenciación tanto en modo inverso como en modo directo, y los dos pueden componerse arbitrariamente en cualquier orden.\n",
    "\n",
    "Lo nuevo es que JAX usa [XLA](https://www.tensorflow.org/xla) para compilar y ejecutar su código NumPy en aceleradores, como GPU y TPU. La compilación ocurre de forma predeterminada, con las llamadas de la biblioteca compiladas y ejecutadas justo a tiempo. Pero JAX incluso le permite compilar justo a tiempo sus propias funciones de Python en núcleos optimizados para XLA utilizando una API de una función. La compilación y la diferenciación automática se pueden componer de forma arbitraria, por lo que puede expresar algoritmos sofisticados y obtener el máximo rendimiento sin tener que abandonar Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade jax jaxlib \n",
    "\n",
    "from __future__ import print_function\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "key = random.PRNGKey(0)\n",
    "# La convención actual es: import numpy original as \"onp\"\n",
    "import numpy as onp\n",
    "import itertools\n",
    "\n",
    "#import random\n",
    "#import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Función de predicción</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x/2)+1)\n",
    "# más estable que  1.0/(1+np.exp(-x))\n",
    "\n",
    "# genera la probabilidad de que una etiqueta sea verdadera\n",
    "def predict(W,b,inputs):\n",
    "    return sigmoid(np.dot(inputs,W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Función de pérdida. Entropía cruzada</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de pérdida: -log de verosimilitud de ejemplos de entrenamiento\n",
    "def loss(W,b,x,y):\n",
    "    preds = predict(W,b,x)\n",
    "    label_probs = preds*y + (1-preds)*(1-y)\n",
    "    return -np.sum(np.log(label_probs))\n",
    "\n",
    "# inicializar coeficientes\n",
    "key, W_key, b_key = random.split(key,3)\n",
    "W = random.normal(key, (3,))\n",
    "b = random.normal(key,())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Ejemplo. Datos de Juguete</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando un dataset de juguete\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Gradiente</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la función *grad* con sus argumentos  para diferenciar la función con respecto a sus parámetros posicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilar con jit\n",
    "# argnums define parámetros posicionales para derivar con respecto a\n",
    "grad_loss = jit(grad(loss,argnums=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_grad =  [ 0.15979266  0.15962079 -1.4914058 ]\n",
      "b_grad =  0.42253572\n"
     ]
    }
   ],
   "source": [
    "W_grad, b_grad = grad_loss(W,b,inputs, targets)\n",
    "print(\"W_grad = \", W_grad)\n",
    "print(\"b_grad = \", b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Entrenamiento del modelo</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de entrenamiento\n",
    "def train(W,b,x,y, lr= 0.12):\n",
    "    gradient = grad_loss(W,b,inputs,targets) \n",
    "    W_grad, b_grad = grad_loss(W,b,inputs,targets)\n",
    "    W -= W_grad*lr\n",
    "    b -= b_grad*lr\n",
    "    return(W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss 2.2908685207366943\n",
      "Epoch 1: train loss 2.0348708629608154\n",
      "Epoch 2: train loss 1.8085304498672485\n",
      "Epoch 3: train loss 1.6108163595199585\n",
      "Epoch 4: train loss 1.4400672912597656\n",
      "Epoch 5: train loss 1.2939282655715942\n",
      "Epoch 6: train loss 1.1695582866668701\n",
      "Epoch 7: train loss 1.0639365911483765\n",
      "Epoch 8: train loss 0.9741388559341431\n",
      "Epoch 9: train loss 0.8975158333778381\n",
      "Epoch 10: train loss 0.8317785263061523\n",
      "Epoch 11: train loss 0.7750089764595032\n",
      "Epoch 12: train loss 0.7256337404251099\n",
      "Epoch 13: train loss 0.6823759078979492\n",
      "Epoch 14: train loss 0.6442046165466309\n",
      "Epoch 15: train loss 0.6102899312973022\n",
      "Epoch 16: train loss 0.5799612998962402\n",
      "Epoch 17: train loss 0.5526753664016724\n",
      "Epoch 18: train loss 0.5279892683029175\n",
      "Epoch 19: train loss 0.5055401921272278\n"
     ]
    }
   ],
   "source": [
    "# entrenamiento\n",
    "weights, biases = [], []\n",
    "train_loss= []\n",
    "epochs = 20\n",
    "\n",
    "train_loss.append(loss(W,b,inputs,targets))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    W,b = train(W,b,inputs, targets)\n",
    "    weights.append(W)\n",
    "    biases.append(b)\n",
    "    losss = loss(W,b,inputs,targets)\n",
    "    train_loss.append(losss)\n",
    "    print(f\"Epoch {epoch}: train loss {losss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "[ 0.94362557 -0.27246025 -0.08247474]\n",
      "[ 0.92442816 -0.2868005   0.08655086]\n",
      "[ 0.90611976 -0.2978541   0.24569045]\n",
      "[ 0.8893553  -0.30661893  0.39487803]\n",
      "[ 0.8745658  -0.31376946  0.5341197 ]\n",
      "[ 0.86198187 -0.31977373  0.66359735]\n",
      "[ 0.8516655  -0.32496306  0.783694  ]\n",
      "[ 0.84355026 -0.3295752   0.8949632 ]\n",
      "[ 0.8374844  -0.33378202  0.9980703 ]\n",
      "[ 0.8332691 -0.3377079  1.0937309]\n",
      "[ 0.83068764 -0.34144276  1.1826608 ]\n",
      "[ 0.8295252  -0.34505108  1.2655417 ]\n",
      "[ 0.82958055 -0.34857863  1.3430017 ]\n",
      "[ 0.8306719  -0.35205755  1.4156077 ]\n",
      "[ 0.8326388  -0.35550994  1.4838635 ]\n",
      "[ 0.83534175 -0.35895056  1.5482135 ]\n",
      "[ 0.83866084 -0.36238888  1.6090478 ]\n",
      "[ 0.8424935  -0.36583066  1.6667081 ]\n",
      "[ 0.84675235 -0.36927888  1.721494  ]\n",
      "[ 0.85136336 -0.3727346   1.7736678 ]\n",
      "biases\n",
      "1.0681342\n",
      "1.018281\n",
      "0.97071755\n",
      "0.9265029\n",
      "0.8863388\n",
      "0.8505914\n",
      "0.81933606\n",
      "0.79242367\n",
      "0.7695556\n",
      "0.75035113\n",
      "0.73439974\n",
      "0.72129667\n",
      "0.7106637\n",
      "0.70215917\n",
      "0.6954815\n",
      "0.6903681\n",
      "0.6865927\n",
      "0.6839612\n",
      "0.68230784\n",
      "0.68149114\n"
     ]
    }
   ],
   "source": [
    "print('weights')\n",
    "for weight in weights:\n",
    "    print(weight)\n",
    "print('biases')\n",
    "for bias in biases:\n",
    "    print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0408358   0.02885852 -0.4149384 ]\n"
     ]
    }
   ],
   "source": [
    "print(grad(loss)(W,b,inputs,targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Calculando el valor de la función y el gradiente con value_and_grad</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value:  0.5055402\n",
      "gradient value:  (DeviceArray([-0.0408358 ,  0.02885852, -0.4149384 ], dtype=float32), DeviceArray(0.00084008, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "from jax import value_and_grad\n",
    "loss_val, Wb_grad = value_and_grad(loss,(0,1))(W,b,inputs, targets)\n",
    "print('loss value: ', loss_val)\n",
    "print('gradient value: ', Wb_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
