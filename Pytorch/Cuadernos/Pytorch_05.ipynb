{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "median-southeast",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:#F72585\">Diferenciación automática con torch.autograd</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-viewer",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-business",
   "metadata": {},
   "source": [
    "Al entrenar redes neuronales, el algoritmo más utilizado es la propagación hacia atrás (**back propagation**). En este algoritmo, los parámetros (pesos del modelo) se ajustan de acuerdo con el gradiente de la función de pérdida con respecto al parámetro dado.\n",
    "\n",
    "Para calcular esos gradientes, PyTorch tiene un motor de diferenciación incorporado llamado `torch.autograd`. Admite el cálculo automático del gradiente para cualquier gráfo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-neighborhood",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Tensores, funciones y grafo computacional</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-queens",
   "metadata": {},
   "source": [
    "Considere la red neuronal de una capa más simple, con entrada $x$, parámetros $w$ y $b$, y alguna función de pérdida. Se puede definir en PyTorch de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "impressive-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # entrada\n",
    "y = torch.Tensor([0, 1, 0]) # etiqueta (valor verdadero)\n",
    "w = torch.randn(5,3, requires_grad=True) # matriz de pesos\n",
    "b = torch.randn(3, requires_grad=True)# bias\n",
    "z = torch.matmul(x,w)+b # calculo de la capa\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee03538e-e212-4df8-9bc5-3728946bf8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=  tensor([1., 1., 1., 1., 1.])\n",
      "y=  tensor([0., 1., 0.])\n",
      "w=  tensor([[-0.6065, -0.7714,  1.8172],\n",
      "        [ 0.2322, -1.9846, -0.3141],\n",
      "        [-0.5862,  0.5827,  1.2384],\n",
      "        [ 0.5976, -0.4063,  0.1319],\n",
      "        [ 0.1880,  1.0771, -0.0722]], requires_grad=True)\n",
      "b=  tensor([-1.5043,  0.4567,  0.2632], requires_grad=True)\n",
      "z=  tensor([-1.6793, -1.0459,  3.0644], grad_fn=<AddBackward0>)\n",
      "loss=  tensor(1.5427, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('x= ', x)\n",
    "print('y= ',y)\n",
    "print('w= ', w)\n",
    "print('b= ', b)\n",
    "print('z= ', z)\n",
    "print('loss= ', loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-rugby",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Entropía Cruzada</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-tuesday",
   "metadata": {},
   "source": [
    "Recuerde que si $p=(p_1, p_2, p_3)$ y $q=(q_1, q_2, q_3)$ son dos distribuciones de probabilidad, de las cuales una se supone la verdadera, digamos $q$ y la otra una aproximación, en este caso $p$., entonces la entropía cruzada entre $p$ y $q$ se define mediante\n",
    "\n",
    "$$\n",
    "\\mathfrak{L}(p,q) = - (q_1 \\log p_1 + q_2 \\log p_2 + q_3 \\log p_3)\n",
    "$$\n",
    "\n",
    "La entropía cruzada mide que tanto se parece la disribución $p$ a la distribución $q$. En el ejemplo se tiene que $p = \\text{softmax(z)}$ y además $q=y$. Observe que por ejemplo $z_1 = x^T w_1$, en donde $w_1$ es la columna 1 de $w$. Note que además $p$ es función de los parámetros $w$ y $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-above",
   "metadata": {},
   "source": [
    "Este código define el siguiente grafo computacional:\n",
    "\n",
    "![](../Imagenes/comp-graph.png)\n",
    "Grafo computacional del cálculo descrito arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-forum",
   "metadata": {},
   "source": [
    "La propiedad `requires_grad` indica que son variables con respecto a las cuales se desea calcular el gradiente de la función *loss*.\n",
    "\n",
    "No es necesario declararlas como tal desde el comienzo. Puede hacerlo luego mediante el método\n",
    "\n",
    "+ x.requires_grad(True).\n",
    "\n",
    "Una función aplicada a tensores de un objeto de la clase `Function`, la cual viene equipada con lo necesario para la diferenciación automática. Una referencia a la función gradiente (back propagation) se almacena en `grad_fn`. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brutal-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000002B4FD0CBD30>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000002B4FD0CBEE0>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-culture",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Cálculo de gradientes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-demonstration",
   "metadata": {},
   "source": [
    "Vamos a calcular $\\frac{\\partial loss} {\\partial w}$ y $\\frac{\\partial loss} {\\partial b}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "practical-recruitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad=  tensor([[ 0.0294, -0.2010,  0.2853],\n",
      "        [ 0.0294, -0.2010,  0.2853],\n",
      "        [ 0.0294, -0.2010,  0.2853],\n",
      "        [ 0.0294, -0.2010,  0.2853],\n",
      "        [ 0.0294, -0.2010,  0.2853]])\n",
      "b.grad=  tensor([ 0.0294, -0.2010,  0.2853])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print('w.grad= ', w.grad)\n",
    "print('b.grad= ', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000a91f",
   "metadata": {},
   "source": [
    "Admonition\n",
    "```{admonition} Nota\n",
    ":class: \n",
    "- Solo podemos obtener las propiedades *grad* para los nodos hoja del grafo computacional, que tienen la propiedad *require_grad* establecida en *True*. Para todos los demás nodos de nuestro gráfico, los degradados no estarán disponibles.\n",
    "- Solo podemos realizar cálculos de gradiente usando hacia atrás una vez en un gráfico dado, por razones de rendimiento. Si necesitamos hacer varias llamadas hacia atrás en el mismo gráfico, debemos pasar *retain_graph = True* a la llamada hacia atrás.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "varied-footwear",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31492/2859123600.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-triple",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Deshabilitar el seguimiento de gradientes</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-direction",
   "metadata": {},
   "source": [
    " De forma predeterminada,  para todos los tensores con *require_grad = True* se está rastreando su historial computacional y admiten el cálculo del gradiente. Sin embargo, hay algunos casos en los que no necesitamos hacer eso, por ejemplo, cuando hemos entrenado el modelo y solo queremos aplicarlo a algunos datos de entrada, es decir, solo queremos hacer cálculos reenviados a través de la red. Podemos detener el seguimiento de los cálculos rodeando nuestro código de cálculo con el bloque  `torch.no_grad ()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-still",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-bahamas",
   "metadata": {},
   "source": [
    "Alternativamente se puede usar el método `detach`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-slovenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-basics",
   "metadata": {},
   "source": [
    "Existen motivos por los que quizás desee deshabilitar el seguimiento de gradientes:\n",
    "\n",
    "- Para marcar algunos parámetros en su red neuronal como parámetros congelados. Este es un escenario muy común para ajustar una red previamente entrenada (finetunning)\n",
    "- Para acelerar los cálculos cuando solo está haciendo un paso hacia adelante, porque los cálculos en tensores que no siguen los gradientes serían más eficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-neighbor",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Más sobre grafos computacionales</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-label",
   "metadata": {},
   "source": [
    "Conceptualmente, *autograd* mantiene un registro de datos (tensores) y todas las operaciones ejecutadas (junto con los nuevos tensores resultantes) en un gráfico acíclico dirigido (DAG) que consta de objetos de tipo *Function*. En este DAG, las hojas son los tensores de entrada, las raíces son los tensores de salida. Al trazar este gráfico desde las raíces hasta las hojas, puede calcular automáticamente los gradientes usando la regla de la cadena.\n",
    "\n",
    "En un paso hacia adelante (foreward), *autograd* hace dos cosas simultáneamente:\n",
    "\n",
    "- ejecuta la operación solicitada para calcular un tensor resultante\n",
    "- mantiene la función de gradiente de la operación en el DAG.\n",
    "\n",
    "El paso hacia atrás(backward) comienza cuando se llama a `.backward()` en la raíz del DAG. `autograd` entonces:\n",
    "\n",
    "- calcula los gradientes de cada `.grad_fn`,\n",
    "- los acumula en el atributo `.grad` del tensor respectivo\n",
    "- utilizando la regla de la cadena, se propaga hasta los tensores de las hojas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3655a586",
   "metadata": {},
   "source": [
    "```{admonition} Nota\n",
    ":class: \n",
    "Los DAG son dinámicos en PyTorch. Una cosa importante a tener en cuenta es que el gráfico se recrea desde cero; después de cada llamada .backward (), autograd comienza a completar un nuevo grafo. Esto es exactamente lo que le permite utilizar declaraciones de flujo de control en su modelo; puede cambiar la forma, el tamaño y las operaciones en cada iteración si es necesario.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-series",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Referencias</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-burden",
   "metadata": {},
   "source": [
    "1. Basado en los [tutoriales de Pytorch](https://pytorch.org/tutorials/)\n",
    "1. [Deep learning for coders with FastAI and Pytorch](http://library.lol/main/F13E85845AE48D9FD7488FE7630A9FD3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
